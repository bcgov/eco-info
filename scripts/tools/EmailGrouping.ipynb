{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sample Script to Sort Emails\n",
    "Source:https://towardsdatascience.com/how-i-used-machine-learning-to-classify-emails-and-turn-them-into-insights-efed37c1e66,https://towardsdatascience.com/how-i-used-machine-learning-to-classify-emails-and-turn-them-into-insights-part-2-6a8f26477c86 https://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and set pandas environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client, pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None,'max_colwidth',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inialize Outlook and access inbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "    \n",
    "inbox = outlook.GetDefaultFolder(6) # \"6\" refers to the index of a folder - in this case,\n",
    "                                        # the inbox. You can change that number to reference\n",
    "                                        # any other folder\n",
    "messages = inbox.Items\n",
    "message = messages.GetFirst()\n",
    "\n",
    "print(message.Categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact inbox and return strings with email information and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emaillist=[]\n",
    "\n",
    "for message in messages:\n",
    "    try:\n",
    "        email= (message.EntryID,message.SenderEmailAddress,message.ReceivedTime,message.subject,message.body,message.Parent,\"unknown\")\n",
    "        emaillist.append(email)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print raw email at index 1 to confirm script worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emaillist[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pandas dataframe and load emaillist, drop empty data, and set date field to datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdraw = pd.DataFrame(emaillist,columns=['EmailID','From','Date','Subject','Body','Parent','Class'])\n",
    "pdraw['Date']=pdraw['Date'].astype('datetime64[ns]')\n",
    "pdraw.dropna(how='all', axis=1)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process body of email\n",
    "1: Split email and take top, ignoring replies\n",
    "2: Remove ministry info, eg ENV:XX\n",
    "3: Remove urls\n",
    "4: Remove email addresses\n",
    "5: Remove special characters\n",
    "6: \n",
    "7: Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdraw['Body']=pdraw['Body'].str.split('\\r\\n\\r\\n \\r\\n\\r\\nFrom').str[0]\n",
    "pdraw['Body']=pdraw['Body'].apply(lambda x:re.sub(r'\\S*:\\S*\\s?', ' ',x))\n",
    "pdraw['Body']=pdraw['Body'].apply(lambda x:re.sub(r'\\S*-\\S-\\S*\\s?', ' ',x))\n",
    "pdraw['Body']=pdraw['Body'].apply(lambda x:re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ',x))\n",
    "pdraw['Body']=pdraw['Body'].apply(lambda x:re.sub(r'\\S*@\\S*\\s?', ' ',x))\n",
    "pdraw['Body']=pdraw['Body'].apply(lambda x:re.sub(r'[^a-zA-Z0-9]+', ' ',x))\n",
    "pdraw['Body']=pdraw['Body'].apply(lambda x:re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", ' ',x))\n",
    "pdraw['Body']=pdraw['Body'].apply(lambda x:re.sub(r' +', ' ',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdraw.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stop word list, vectorize words to determine frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(WordNetLemmatizer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = text.ENGLISH_STOP_WORDS.union(['deepa','brett','corey','jackie','gillian','auger','external','filatow','erwin'])\n",
    "\n",
    "#dictionairy = ['gis','soil','course','map','job']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords,analyzer='word',strip_accents='unicode')\n",
    "   \n",
    "x= vectorizer.fit_transform(pdraw['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph results of vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dense = x.todense()\n",
    "coords = PCA(n_components=2).fit_transform(x_dense)\n",
    "\n",
    "plt.scatter(coords[:,0], coords[:,1],c='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Cluster above scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(x_dense)     \n",
    "\n",
    "y_means = kmeans.predict(x_dense)\n",
    "scatter = plt.scatter(coords[:,0], coords[:,1],c=y_means)\n",
    "plt.colorbar(scatter,spacing='uniform')\n",
    "plt.rcParams['figure.dpi']=100\n",
    "\n",
    "\n",
    "\n",
    "cluster_map = pd.DataFrame()\n",
    "cluster_map['data_index']=pdraw.index.values\n",
    "cluster_map['cluster']= kmeans.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Clustering to pdraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdcluster = pdraw.join(cluster_map,lsuffix=pdraw.index.values,rsuffix=cluster_map['data_index'])\n",
    "pdcluster.sort_values(['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to get most frequent words from each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=20):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats, columns=['features', 'score'])\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(X, features, row_id, top_n=25):\n",
    "    row = np.squeeze(X[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returns top X most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "print (top_feats_in_doc(x, features, 1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to aggregate top words in all emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mean_feats(X, features,grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    if grp_ids:\n",
    "        D = X[grp_ids].toarray()\n",
    "    else:\n",
    "        D = X.toarray()\n",
    "        \n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print top 50 results in all emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (top_mean_feats(x, features, top_n=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new vectorizer to feed into cosine relationship, input query term, vectorize query term. Values that are closer to query will have higher result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'bctw'\n",
    "\n",
    "vec_query = vectorizer.transform([query])\n",
    "\n",
    "cosine_sim = linear_kernel(vec_query,x_dense).flatten()\n",
    "\n",
    "x=np.count_nonzero(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print index of emails that are related to search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_email_indices = cosine_sim.argsort()[:-x:-1]\n",
    "print(related_email_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create Dictionary with emailid and queryword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emaildict={}\n",
    "\n",
    "for item in related_email_indices:\n",
    "    dictvalue=pdraw.loc[item,'EmailID']\n",
    "    emaildict[dictvalue]=query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages:\n",
    "    for key in emaildict:\n",
    "        if message.EntryID==key:\n",
    "            message.Categories=emaildict.get(key)\n",
    "            message.Save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For related emails, print details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emails in related_email_indices:\n",
    "    print(pdraw.values[emails])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
